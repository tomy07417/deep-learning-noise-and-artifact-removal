# üé• Eliminaci√≥n de Manchas Fijas en Videos usando U-Net

Un sistema de deep learning para remover manchas est√°ticas en videos (como las causadas por suciedad en la lente) aprovechando el movimiento temporal como informaci√≥n de reconstrucci√≥n.

---

## üìã Tabla de Contenidos

1. [Descripci√≥n del Problema](#descripci√≥n-del-problema)
2. [Idea Principal](#idea-principal)
3. [Arquitectura del Modelo](#arquitectura-del-modelo)
4. [Dataset](#dataset)
5. [Funcionalidades](#funcionalidades)
6. [Estructura del Proyecto](#estructura-del-proyecto)
7. [Instalaci√≥n y Uso](#instalaci√≥n-y-uso)
8. [Resultados](#resultados)
9. [Mejoras Futuras](#mejoras-futuras)

---

## üìå Descripci√≥n del Problema

### El Desaf√≠o

Cuando una lente tiene una mancha fija (polvo, humedad, ray√≥n, etc.), esa imperfecci√≥n aparece en **todos los frames** del video. Sin embargo, el contenido detr√°s de la mancha **s√≠ se mueve**.

**Ejemplo visual:**
```
Frame 1: D√≠gito en posici√≥n A + mancha en (x, y)
Frame 2: D√≠gito en posici√≥n B + mancha en (x, y)  ‚Üê misma mancha, contenido diferente
Frame 3: D√≠gito en posici√≥n C + mancha en (x, y)  ‚Üê podemos inferir qu√© hay detr√°s
```

### Por Qu√© es Posible Resolver Esto

Si analizamos varios frames consecutivos, la **informaci√≥n temporal** nos permite reconstruir lo que est√° ocultado:

- La mancha siempre est√° en el **mismo p√≠xel (x, y)** en todos los frames
- El contenido detr√°s de la mancha **cambia** de un frame al siguiente
- Con suficientes frames, tenemos "ventanas" donde diferentes partes del contenido se ven alrededor de la mancha

Este es el n√∫cleo de la soluci√≥n: **el movimiento del contenido proporciona la informaci√≥n que necesitamos para reconstruir las √°reas ocultas**.

---

## üí° Idea Principal

El modelo toma un **frame corrupto** (con mancha) y genera un **frame restaurado** (sin mancha). Lo inteligente es que:

1. **El modelo ve un solo frame** ‚Üí debe aprender que la mancha es an√≥mala
2. **El temporal smoothing** ‚Üí garantiza coherencia entre frames consecutivos
3. **En conjunto**: El video reconstruido es fluido y visualmente coherente

### Pipeline Completo

```
Video Original ‚Üí Agregar Mancha ‚Üí Entrenar Modelo ‚Üí Video Reconstruido
    (limpio)     (sintetizada)      (U-Net + TS)    (sin mancha)
```

---

## üèóÔ∏è Arquitectura del Modelo

### U-Net: Arquitectura Encoder-Decoder

La **U-Net** es una arquitectura dise√±ada espec√≠ficamente para tareas de **restauraci√≥n y segmentaci√≥n pixel-a-pixel**.

#### Estructura General

```
Entrada: (B, C_in, H, W)  ‚Üí (B, 3, 256, 256)

ENCODER (Contracci√≥n)
‚îú‚îÄ‚îÄ Initial Conv      ‚Üí (B, 64, 256, 256)
‚îú‚îÄ‚îÄ Down Conv 1       ‚Üí (B, 128, 128, 128)   [MaxPool 2x2]
‚îú‚îÄ‚îÄ Down Conv 2       ‚Üí (B, 256, 64, 64)     [MaxPool 2x2]
‚îú‚îÄ‚îÄ Down Conv 3       ‚Üí (B, 512, 32, 32)     [MaxPool 2x2]
‚îî‚îÄ‚îÄ Down Conv 4       ‚Üí (B, 1024, 16, 16)    [MaxPool 2x2]

BOTTLENECK (Cuello de botella)
‚îî‚îÄ‚îÄ (B, 1024, 16, 16)

DECODER (Expansi√≥n con conexiones residuales)
‚îú‚îÄ‚îÄ Up Conv 1 + Skip from Down 3   ‚Üí (B, 512, 32, 32)   [Upsample 2x2]
‚îú‚îÄ‚îÄ Up Conv 2 + Skip from Down 2   ‚Üí (B, 256, 64, 64)   [Upsample 2x2]
‚îú‚îÄ‚îÄ Up Conv 3 + Skip from Down 1   ‚Üí (B, 128, 128, 128) [Upsample 2x2]
‚îî‚îÄ‚îÄ Up Conv 4 + Skip from Initial  ‚Üí (B, 64, 256, 256)  [Upsample 2x2]

Final Conv
‚îî‚îÄ‚îÄ Salida: (B, C_out, H, W)  ‚Üí (B, 3, 256, 256)
```

#### Componentes Clave

**1. DoubleConv: Bloque de Convoluci√≥n Doble**
```python
Conv(3x3) ‚Üí BatchNorm ‚Üí ReLU
    ‚Üì
Conv(3x3) ‚Üí BatchNorm ‚Üí ReLU
```
Permite aprender caracter√≠sticas m√°s complejas con menos par√°metros.

**2. Skip Connections (Conexiones Residuales)**
```
Encoder: x‚ÇÅ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚Üì                       ‚îÇ
         x‚ÇÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
          ‚Üì              ‚îÇ        ‚îÇ
         x‚ÇÉ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ        ‚îÇ
          ‚Üì       ‚îÇ      ‚îÇ        ‚îÇ
        [Bottleneck]     ‚îÇ        ‚îÇ
          ‚Üì       ‚îÇ      ‚îÇ        ‚îÇ
         u‚ÇÅ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ        ‚îÇ
          ‚Üì              ‚îÇ        ‚îÇ
         u‚ÇÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
          ‚Üì                       ‚îÇ
         u‚ÇÉ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

Las **skip connections** permiten que informaci√≥n de alta resoluci√≥n del encoder llegue directamente al decoder, evitando p√©rdida de detalles.

**3. DownConv: Codificaci√≥n (compresi√≥n)**
- MaxPool 2x2 para reducir dimensiones
- DoubleConv para extraer caracter√≠sticas
- Reduce espacialmente, aumenta canales

**4. UpConv: Decodificaci√≥n (expansi√≥n)**
- Upsample 2x2 (bic√∫bico) para aumentar resoluci√≥n
- Conv 1x1 para ajustar canales
- Concatenaci√≥n (cat) con skip connections
- DoubleConv para procesar la fusi√≥n

#### Configuraci√≥n del Modelo

```python
CHANNELS_IN = 3        # RGB
CHANNELS = 64          # Base channels

Modelo:
‚îú‚îÄ‚îÄ Initial Conv:  3 ‚Üí 64
‚îú‚îÄ‚îÄ Down Conv 1:   64 ‚Üí 128
‚îú‚îÄ‚îÄ Down Conv 2:   128 ‚Üí 256
‚îú‚îÄ‚îÄ Down Conv 3:   256 ‚Üí 512
‚îú‚îÄ‚îÄ Down Conv 4:   512 ‚Üí 1024
‚îú‚îÄ‚îÄ Up Conv 1:     1024 ‚Üí 512
‚îú‚îÄ‚îÄ Up Conv 2:     512 ‚Üí 256
‚îú‚îÄ‚îÄ Up Conv 3:     256 ‚Üí 128
‚îú‚îÄ‚îÄ Up Conv 4:     128 ‚Üí 64
‚îî‚îÄ‚îÄ Final Conv:    64 ‚Üí 3 (RGB)

Par√°metros totales: ~13.4M
```

---

### Temporal Smoothing: Coherencia en el Tiempo

Despu√©s de que la U-Net procesa cada frame por separado, aplicamos **temporal smoothing** para garantizar coherencia entre frames consecutivos.

#### El Problema: Flickering

Aunque todos los frames sean casi id√©nticos:
- Frame t ‚Üí Red output = [127.3, 128.1, 126.8]
- Frame t+1 ‚Üí Red output = [127.5, 129.2, 127.1]
- Frame t+2 ‚Üí Red output = [126.8, 127.9, 126.5]

Las peque√±as variaciones crean un efecto de **parpadeo visual (flickering)** inc√≥modo.

#### La Soluci√≥n: Promediado Temporal Ponderado

```
smoothed[t] = strength √ó frame[t] + 
              ((1 - strength) / 2) √ó frame[t-1] + 
              ((1 - strength) / 2) √ó frame[t+1]
```

Con `strength = 0.6`:
```
smoothed[t] = 0.6 √ó frame[t] + 0.2 √ó frame[t-1] + 0.2 √ó frame[t+1]
```

**Ventajas:**
- Mantiene el frame central como referencia (60%)
- Promedia con vecinos para suavidad (40% distribuido)
- Los bordes (frame 0 y √∫ltimo) se mantienen sin suavizar
- Reduce ruido y variaciones abruptas

**Resultado:** Videos m√°s fluidos y coherentes visualmente.

---

## üìä Dataset

### Fuente: MNIST Animado

**Dataset original:** [Captioned Moving MNIST - Medium Version](https://www.kaggle.com/datasets/yichengs/captioned-moving-mnist-dataset-medium-version)

**Caracter√≠sticas:**
- D√≠gitos manuscritos (0-9) moviendo aleatoriamente en un canvas
- Movimiento consistente y predecible
- Fondo simple y uniforme
- Ideal para prototiping de modelos de visi√≥n temporal

### Generaci√≥n del Dataset de Entrenamiento

El proceso est√° automatizado en `generacion_dataset.ipynb`:

#### Paso 1: Corte de Videos
```python
Duraci√≥n original: ~60 segundos
Duraci√≥n corte: 15 segundos (360 frames @ 24fps)
Raz√≥n: Datos m√°s manejables, reduce almacenamiento
```

#### Paso 2: S√≠ntesis de Manchas

**Funci√≥n `generate_circular_stain()`:**
```python
def generate_circular_stain(h, w, radius=25, opacity=0.7, hardness=0.8):
    # 1. Centro aleatorio dentro del frame
    cx, cy = random(0, w), random(0, h)
    
    # 2. M√°scara gaussiana suave (hardness controla el borde)
    dist = sqrt((x - cx)¬≤ + (y - cy)¬≤) / radius
    mask = clip(1 - dist, 0, 1) ^ hardness
    
    # 3. Aplicar opacidad (0.0 = invisible, 1.0 = opaca)
    mask = mask * opacity
    
    return mask
```

**Par√°metros controlables:**
- `radius`: Tama√±o de la mancha (en p√≠xeles)
- `opacity`: Intensidad de opacidad (0.7-1.0)
- `hardness`: Suavidad del borde (0.5 = muy suave, 1.0 = abrupto)

#### Paso 3: Aplicaci√≥n de la Mancha

```python
def apply_stain_to_video(frames, stain_mask):
    for frame in frames:
        # Normalizar frame a [0, 1]
        frame_normalized = frame / 255.0
        
        # Combinar: darkening + tinting
        corrupted = frame_normalized √ó (1 - mask) + (0.3 √ó mask)
        
        # Oscurece la zona de la mancha con un tinte
        return clip(corrupted, 0, 1) √ó 255
```

#### Paso 4: M√∫ltiples Manchas

Para mayor variedad, se agregan **0 a 2 manchas adicionales** por video:
```python
can_manchas = random(0, 2)
for _ in range(can_manchas):
    mask = generate_circular_stain(...)
    video = apply_stain_to_video(video, mask)
```

### Estructura Final del Dataset

```
dataset/
‚îú‚îÄ‚îÄ batch_0/
‚îÇ   ‚îú‚îÄ‚îÄ video_0/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ video_original.mp4      (sin mancha)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ video_con_manchas.mp4   (con mancha/s)
‚îÇ   ‚îú‚îÄ‚îÄ video_1/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ video_original.mp4
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ video_con_manchas.mp4
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ batch_1/
‚îÇ   ‚îî‚îÄ‚îÄ ... (similar)
‚îî‚îÄ‚îÄ ...
```

**Estad√≠sticas:**
- Videos por batch: ~60
- Total batches: 20
- Videos totales: ~1200
- Duraci√≥n: 15 segundos c/u @ 24fps = 360 frames c/u

### Divisi√≥n Train/Val

```python
TRAIN_SIZE = 80% de dataset
VAL_SIZE = 20% de dataset

DataLoader:
‚îú‚îÄ‚îÄ Batch size: 64
‚îú‚îÄ‚îÄ Shuffle: True (train), False (val)
‚îî‚îÄ‚îÄ num_workers: 0 (CPU), pin_memory: True (GPU)
```

---

## ‚öôÔ∏è Funcionalidades

### 1. **Generaci√≥n del Dataset** (`generacion_dataset.ipynb`)

Automatiza la creaci√≥n del dataset de entrenamiento:
- Descarga videos MNIST animados
- Corta a duraci√≥n consistente (15s)
- Genera manchas sint√©ticas realistas
- Crea pares input/target para supervision

**Entrada:** Videos originales en `./mmnist-medium/`  
**Salida:** Dataset estructura en `./dataset/`

### 2. **Entrenamiento del Modelo** (`trainning_model.ipynb`)

Entrena la U-Net con los datos generados:

```python
# Configuraci√≥n
CHANNELS_IN = 3
CHANNELS = 64
BATCH_SIZE = 64
LR = 1e-4
EPOCHS = 100

# Loss function
Loss = L1Loss (MAE)
# M√°s robusto a outliers que MSE

# Optimizer
Optimizer = Adam(lr=1e-4)

# Early Stopping
patience = 7 epochs sin mejora
```

**Monitoreo:**
- Train Loss
- Validation Loss
- Checkpoints cada 10 epochs
- Best model guardado autom√°ticamente

### 3. **Prueba del Modelo** (`test_model.ipynb`)

Demuestra el pipeline completo:

1. **Lectura de video:** `read_video(filename)`
   - Carga todos los frames
   - Convierte BGR ‚Üí RGB

2. **S√≠ntesis de manchas:** `generate_circular_stain()`
   - Crea manchas realistas
   - M√∫ltiples manchas por video

3. **Aplicaci√≥n de manchas:** `apply_stain_to_video()`
   - Oscurece zonas
   - Mantiene variabilidad

4. **Procesamiento frame-a-frame:**
   ```python
   for frame in corrupted_video:
       # Normalizar
       frame = RGB2Tensor / 255.0
       
       # Padding a m√∫ltiplo de 16 (requerimiento de U-Net)
       frame = pad_to_multiple(frame, 16)
       
       # Inferencia
       with no_grad():
           restored = model(frame)
       
       # Post-procesamiento
       restored = clip(restored √ó 255, 0, 255)
   ```

5. **Temporal Smoothing:** `temporal_smooth(frames, strength=0.6)`
   - Reduce flickering
   - Mejora coherencia temporal

6. **Guardado:** `save_video(frames, filename, fps=24)`
   - Escribe con codec mp4v
   - Preserva duraci√≥n original

### 4. **Visualizaci√≥n** (`visualizacion_videos.ipynb`)

Herramientas para inspeccionar resultados:
- Reproductor interactivo de videos
- Comparaci√≥n original vs corrupto vs restaurado
- Generaci√≥n de manchas con par√°metros ajustables

---

## üìÅ Estructura del Proyecto

```
./
‚îú‚îÄ‚îÄ unet.py                         # Definici√≥n del modelo
‚îÇ   ‚îú‚îÄ‚îÄ Conv3K                      # Conv 3x3
‚îÇ   ‚îú‚îÄ‚îÄ DoubleConv                  # Bloque doble conv
‚îÇ   ‚îú‚îÄ‚îÄ DownConv                    # Encoder block
‚îÇ   ‚îú‚îÄ‚îÄ UpConv                      # Decoder block
‚îÇ   ‚îú‚îÄ‚îÄ UNet                        # Arquitectura completa
‚îÇ   ‚îî‚îÄ‚îÄ temporal_smooth()           # Post-procesamiento
‚îÇ
‚îú‚îÄ‚îÄ generacion_dataset.ipynb        # Crear dataset
‚îú‚îÄ‚îÄ trainning_model.ipynb           # Entrenar U-Net
‚îú‚îÄ‚îÄ test_model.ipynb                # Inferencia y validaci√≥n
‚îú‚îÄ‚îÄ visualizacion_videos.ipynb      # Herramientas de visualizaci√≥n
‚îÇ
‚îú‚îÄ‚îÄ dataset/                        # Dataset generado
‚îÇ   ‚îú‚îÄ‚îÄ batch_0/
‚îÇ   ‚îú‚îÄ‚îÄ batch_1/
‚îÇ   ‚îî‚îÄ‚îÄ ... (20 batches)
‚îÇ
‚îú‚îÄ‚îÄ mmnist-medium/                  # Videos originales MNIST
‚îÇ   ‚îú‚îÄ‚îÄ batch_0_video_0.mp4
‚îÇ   ‚îú‚îÄ‚îÄ batch_0_video_1.mp4
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ best_model.pth                  # Mejor modelo entrenado
‚îú‚îÄ‚îÄ checkpoint_unet_10.pth          # Checkpoints
‚îú‚îÄ‚îÄ checkpoint_unet_20.pth
‚îî‚îÄ‚îÄ ...
```

---

## üöÄ Instalaci√≥n y Uso

### Requisitos

```
Python >= 3.8
PyTorch >= 1.9 (recomendado con CUDA support)
opencv-python >= 4.5
numpy >= 1.19
matplotlib >= 3.3
```

### Instalaci√≥n

```bash
# 1. Clonar/descargar el proyecto
cd /home/tomy07417/data-science/opcional

# 2. Crear entorno virtual (recomendado)
python -m venv venv
source venv/bin/activate

# 3. Instalar dependencias
pip install -r requirements.txt
```

### Uso Paso a Paso

#### Opci√≥n 1: Entrenar desde Cero

```bash
# 1. Generar dataset
jupyter notebook generacion_dataset.ipynb
# Ejecutar todas las celdas

# 2. Entrenar modelo
jupyter notebook trainning_model.ipynb
# Configurar par√°metros si es necesario
# Ejecutar entrenamiento (puede tomar varias horas)

# Resultado: best_model.pth
```

#### Opci√≥n 2: Usar Modelo Pre-entrenado

```bash
# 1. Usar test_model.ipynb directamente
jupyter notebook test_model.ipynb

# 2. Modificar video de entrada:
# L√≠nea 2: frames = read_video("./tu_video.mp4")

# 3. Ejecutar celdas en orden
# Resultado: video_reconstruido.mp4
```

#### Opci√≥n 3: Script Personalizado

```python
import torch
import cv2
import numpy as np
from unet import UNet, temporal_smooth

# Cargar modelo
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = UNet(3, 64).to(device)
model.load_state_dict(torch.load("best_model.pth"))
model.eval()

# Procesar video
cap = cv2.VideoCapture("video_input.mp4")
frames_out = []

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # Normalizar y convertir
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0
    frame_tensor = torch.from_numpy(frame_rgb).permute(2, 0, 1).unsqueeze(0).to(device)
    
    # Padding
    _, H, W = frame_tensor.shape[1:]
    pad_h = (16 - H % 16) % 16
    pad_w = (16 - W % 16) % 16
    if pad_h > 0 or pad_w > 0:
        frame_tensor = torch.nn.functional.pad(frame_tensor, (0, pad_w, 0, pad_h))
    
    # Inferencia
    with torch.no_grad():
        output = model(frame_tensor)
    
    # Desnormalizar
    output_frame = output.squeeze(0).permute(1, 2, 0).cpu().numpy()
    output_frame = np.clip(output_frame * 255, 0, 255).astype(np.uint8)
    
    frames_out.append(output_frame)

cap.release()

# Suavizado temporal y guardado
frames_smooth = temporal_smooth(np.array(frames_out), strength=0.6)
# ... guardar video
```

---

## üìà Resultados

### M√©tricas de Entrenamiento

Monitoreadas durante el entrenamiento:
- **Train Loss:** Disminuye constantemente (modelo aprendiendo)
- **Validation Loss:** Valida generalizaci√≥n
- **Early Stopping:** Si val_loss no mejora en 7 epochs

### Ejemplos Visuales

**Video Original (limpio)**
- MNIST d√≠gito movi√©ndose
- Fondo uniforme

**Video Corrupto (con mancha)**
- Mancha circular oscura fija
- D√≠gito se mueve detr√°s

**Video Restaurado (salida modelo)**
- Mancha removida por U-Net
- Temporal smoothing elimina flickering
- Reconstrucci√≥n clara del movimiento

---

## üîÆ Mejoras Futuras

### Corto Plazo

1. **Arquitecturas Alternativas**
   - ResNet para baseline
   - DenseNet para mejor flujo de caracter√≠sticas
   - Attention mechanisms para enfoque en manchas

2. **Variabilidad de Manchas**
   - Manchas irregulares (no solo c√≠rculos)
   - Manchas que var√≠an en intensidad
   - M√∫ltiples manchas con diferentes opacidades

3. **Optimizaci√≥n**
   - Reducir tama√±o del modelo
   - Quantizaci√≥n para inferencia m√°s r√°pida
   - TorchScript para deployment

### Mediano Plazo

4. **Modelos Temporales**
   - ConvLSTM para procesar secuencias
   - 3D-UNet que vea m√∫ltiples frames simult√°neamente
   - Transformers para relaciones de largo plazo

5. **Datos Reales**
   - Recolectar videos reales con manchas
   - Fine-tuning con datos sint√©ticos + reales
   - Dataset de diferentes tipos de defectos

6. **Video M√°s Complejo**
   - Escenas naturales (no solo MNIST)
   - M√∫ltiples objetos
   - Oclusiones y movimientos r√°pidos

### Largo Plazo

7. **Arquitecturas de Estado del Arte**
   - Blind inpainting con GANs
   - Diffusion models para reconstrucci√≥n
   - Multi-scale processing

8. **Aplicaciones Real-World**
   - Streaming en vivo
   - C√°maras de vigilancia
   - Procesamiento de video hist√≥rico
   - Restauraci√≥n de filmograf√≠a antigua

---

## üìö Referencias

- **U-Net Paper:** [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)
- **Dataset:** [Moving MNIST Dataset - Kaggle](https://www.kaggle.com/datasets/yichengs/captioned-moving-mnist-dataset-medium-version)
- **PyTorch Docs:** [pytorch.org](https://pytorch.org/)

---

## üë®‚Äçüíº Autor

**Tom√°s Amundarain**  
TP N¬∞4 - Eliminaci√≥n de Manchas en Videos  
Diciembre 2025

---

## üìù Licencia

Este proyecto es de c√≥digo abierto para prop√≥sitos educativos.
